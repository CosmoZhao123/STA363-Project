---
title: "STA363: Project"
author: "Sihang Zhao, Haoxuan Sun"
date: "2026-02-19"
output:
  pdf_document:
    toc: true
    toc_depth: 3
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message= FALSE, fig.asp = .6)
```


```{r}
library(ggplot2)
library(rpart)
library(rattle)
library(rpart.plot)
library(nnet)
library(dplyr)
library(MASS)
library(tidyr)
library(naivebayes)
library(corrplot)
library(caret)
library(knitr)
library(kableExtra)

mydata<-read.csv('/Users/cosmozhao/Desktop/NBA Player Salaries (2022-23 Season).csv')


```

\newpage
# EXECUTIVE SUMMARY AND CONCLUSION


\newpage
# AI STATEMENT
No generative AI tools were used in the process of conducting the analysis, performing the statistical modeling, or writing the substantive content of this report. All data analysis, coding, interpretation, and written explanations were completed independently by me, Sihang Zhao, as the sole author of this work. The only digital tool used during the writing process was Grammarly, and it was used exclusively for minor grammar and spelling corrections.


\newpage
# Project Title: 
# INTRODUCTION


# ANALYSIS PROCEDURES


\newpage
\vspace*{\fill}
\begin{center}
\textbf{RESULTS ARE SHOWN BELOW}
\end{center}
\vspace*{\fill}


\newpage
# Section 1: Data and Goal Overview

For this analysis, we use a dataset containing NBA player statistics and salaries from the 2022–2023 season. The dataset consists of (467) observations and (52) variables, where each observation represents an individual NBA player. The response variable is player salary, measured in U.S. dollars. While the dataset includes categorical variables such as player name, team, and position, these variables are excluded from the modeling process. Only quantitative performance statistics are used as predictors.

The goal of this project is to predict a player’s salary based solely on measurable on-court performance metrics. By focusing on numerical variables, we aim to assess the extent to which statistical productivity explains differences in player compensation.

The primary features considered include age, games played (GP), games started (GS), minutes per game (MP), field goals (FG), field goal percentage (FG%), three-point statistics, rebounds, assists, steals, blocks, turnovers, points per game, and other continuous box score or advanced metrics available in the dataset. These variables capture scoring efficiency, playing time, and overall player contribution.

This project is particularly interesting because salary determination in professional sports reflects performance valuation within a competitive market. By modeling salary as a function of quantitative performance indicators, we can evaluate how strongly observable productivity translates into financial compensation.



# Section 2: EDA

## Raw Data Glimpse
```{r}
# build d inside this chunk (so knitting always has it)
d <- mydata %>%
  dplyr::select(-X) %>%                             # remove X if it exists
  dplyr::mutate(dplyr::across(where(is.numeric), ~ round(.x, 2))) %>%
  head(6)

key <- c("Player.Name", "Salary")   # keep these visible in both tables
p <- 10                              # columns per table (adjust)

# split columns into blocks of size p
col_blocks <- split(names(d), ceiling(seq_along(names(d)) / p))

# Part A
d %>%
  dplyr::select(any_of(key), all_of(setdiff(col_blocks[[1]], key))) %>%
  kable(booktabs = TRUE, caption = "Raw data preview (Part A)") %>%
  kable_styling(latex_options = "scale_down")

# Part B (only if exists)
if (length(col_blocks) > 1) {
  d %>%
    dplyr::select(any_of(key), all_of(setdiff(col_blocks[[2]], key))) %>%
    kable(booktabs = TRUE, caption = "Raw data preview (Continued)") %>%
    kable_styling(latex_options = "scale_down")
}
```

As shown in Table 1 & 2 above, our raw data includes in total of (22) columns, with (1) response variable (Y: Salary in US Dollar), and (21) features (3 categorical and 18 numerical). However, there are some issues within our feature set, particularly potential multicollinearity among the predictors. Therefore, we are going to conduct a multicollinearity check across all features and remove those with strong correlations with each other.

Additionally, ‘Player Name’, ‘Position’, and ‘Team’ are not useful categorical variables for predicting salary and may introduce unnecessary noise into the model. Therefore, we will also remove them during our data cleaning process.

## Check for multicollinearity among features

```{r,fig.asp = 1}
# Select numeric variables only
num_data <- mydata %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(-any_of("X"))   # remove index column if present

# Compute correlation matrix
cor_mat <- cor(num_data, use = "complete.obs")

# Plot correlogram
corrplot(cor_mat,
         method = "circle",
         type = "upper",
         order = "hclust",
         tl.col = "black",
         tl.cex = 0.6)

```

As shown in Figure 1 above, many features exhibit strong correlations with one another. For example, Field Goals Made is highly correlated with Points, and Minutes Played is strongly correlated with both Field Goals Made and Points. Three-Point Attempts also shows a strong correlation with Three-Point Field Goals Made. In addition, Total Rebounds is closely related to Blocks, and Free Throws Made is strongly associated with Free Throw Attempts.

Such high correlations across predictors may lead to multicollinearity, which can inflate variance, destabilize coefficient estimates, and reduce model interpretability. Therefore, we are going to remove features that show strong correlations with each other in order to reduce redundancy and improve model stability.

## The Data 

```{r}
mydata_clean <- mydata %>%
  dplyr::select(
    Salary,
    Age,
    MP,       # choose this instead of PTS
    AST.,
    TRB,
    BPM,
    TOV.
  )

knitr::kable(mydata_clean[1:10,], caption = "The First 10 Rows of Our Data")
```

After data processing, we selected the (6) features that most strongly affect our prediction while showing no strong correlation with each other, and created the final dataset used in this report. As shown above in Table 3, we present the first ten rows of our cleaned dataset.

To help our client better understand the acronyms used in the dataset, Table 4 provides the full names and definitions of each variable included in the model.

For the complete dataset used in this analysis, please refer to [Appendix A](#appendix-a) at the end of this report.

```{r}
glossary <- data.frame(
  Acronym = c("Salary", "Age", "MP", "AST", "TRB", "BPM", "TOV"),
  Explanation = c(
    "Annual player salary (USD)",
    "Player age (years)",
    "Minutes played per game",
    "Assists per game",
    "Total rebounds per game",
    "Box Plus/Minus — overall impact metric",
    "Turnovers per game"
  )
)

kable(glossary,
      booktabs = TRUE,
      col.names = c("Acronym", "Explanation"),
      caption = "Variable Definitions") %>%
  kable_styling(latex_options = "hold_position")
```

## Data Exploration

### Research Question 1: How does the players' salary change over MP (Minutes played Per game)?

```{r}
ggplot(mydata_clean, aes(x = MP, y = Salary)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(x = "Minutes played Per game (MP)", y = "Salary ($)", caption = "Figure 2: The relationship between minutes played per game and salary")
```

As shown in Figure 2 above, although there is a clear positive relationship between Minutes Played per game and Salary, the pattern does not appear strictly linear. The spread of the data increases as Minutes Played increases, indicating potential heteroskedasticity, and the relationship appears slightly curved rather than perfectly linear. This suggests that a linear model on the raw Salary variable may not be the most appropriate specification.

Therefore, it seems that applying a logarithmic transformation to Salary may provide a better fit. By modeling log(Salary) instead of Salary, we can stabilize the variance, reduce skewness in the response variable, and potentially achieve a more linear relationship between Minutes Played and Salary. Consequently, we will repeat the same analysis using log(Salary) as the response variable.

```{r}
ggplot(mydata_clean, aes(x = MP, y = log(Salary))) + 
  geom_point() + 
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(x = "Minutes played Per game (MP)", y = "Log(Salary) ($)", caption = "Figure 3: The relationship between minutes played per game and log salary")
```

The log transformation appears to substantially improve the linearity of the relationship between Minutes Played per game and Salary. As shown in Figure 3, when modeling log(Salary) against Minutes Played, the data points align more closely around the fitted regression line compared to the raw Salary model.

In addition, the variance of the residuals appears more stable across different levels of Minutes Played. The relationship now shows a clearer linear trend, suggesting that a linear regression model with log(Salary) as the response variable is more appropriate than using raw Salary.

Based on the scatterplot and the fitted regression line, players’ salary increases as Minutes Played per game increases. There is a clear positive relationship between playing time and compensation: players who average more minutes per game tend to earn higher salaries.

When examining the raw Salary model, the relationship appears upward sloping but exhibits increasing variability at higher levels of Minutes Played, indicating potential nonlinearity. After applying a logarithmic transformation to Salary, the relationship becomes more linear and the variance appears more stable. The fitted regression line in the log(Salary) model confirms a strong positive association between Minutes Played and salary.

Interpreting the log-scale model, this suggests that each additional minute played per game is associated with a proportional (percentage) increase in salary rather than a constant dollar increase.

## Section 3

```{r}
compute_RMSE <- function(actual, prediction) {
  error <- actual - prediction
  rmse <- sqrt(mean(error^2))
  return(rmse)
}

##LOOCV
Kchoice<-data.frame("Choice of K"=2:50,"RMSE"=rep(NA,49))
n<-nrow(mydata_clean)
for(k in 2:50){
  yhat<-rep(NA,n)
  for(i in 1:n){
    train<-mydata_clean[-i,]
    validation<-mydata_clean[i,]
    yhat[i]<-knnregTrain(train[,-1],
                         validation[,-1],
                         train[,1],
                         k=k)
  }
  Kchoice[k-1,"RMSE"]<-compute_RMSE(mydata_clean$Salary,yhat)
}


## 5 fold CV
Kchoice2<-data.frame("Choice of K"=2:50,"RMSE"=rep(NA,49))
folds<-rep(1:5,n/5+1)
fold<-sample(folds,n,replace=FALSE)
for(k in 2:50){
  yhat<-rep(NA,n)
  for(i in 1:5){
    infold<-which(fold==i)
    train<-mydata_clean[-infold,]
    validation<-mydata_clean[infold,]
    yhat[infold]<-knnregTrain(train[,-1],
                         validation[,-1],
                         train[,1],
                         k=k)
  }
  Kchoice2[k-1,"RMSE"]<-compute_RMSE(mydata_clean$Salary,yhat)
}


plot_data <- data.frame(
  Actual = mydata_clean$Salary,
  Predicted = yhat
)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point()
mean(mydata_clean$Salary)
```



\newpage
# Appendix A: The Data {#appendix-a}

```{r}
knitr::kable(mydata_clean, title = "The Original Data")
```


